{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from collections import Counter, defaultdict\n",
    "import os\n",
    "from random import shuffle\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "num_sentences = 2000\n",
    "\n",
    "def load_sst_data(path):\n",
    "    data = []\n",
    "    with open(path) as f:\n",
    "        for i, line in enumerate(f): \n",
    "            example = {}\n",
    "            text = re.sub(r'\\s*(\\(\\d)|(\\))\\s*', '', line)\n",
    "            example['text'] = text[0:]\n",
    "            data.append(example)\n",
    "            if i > num_sentences:\n",
    "                break\n",
    "    return data\n",
    "     \n",
    "training_set = load_sst_data('/scratch/qx344/data5/training-data/training-data.1m')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def readEvalVocab(path):\n",
    "    Vocab = []\n",
    "    with open(path) as f:\n",
    "        for i, line in enumerate(f):\n",
    "            Vocab.append(line[:-2])\n",
    "            #print(line[:-1])\n",
    "    return Vocab\n",
    "evalVocab = readEvalVocab('/scratch/qx344/data5/training-data/evalVocab.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = training_set[0]['text']\n",
    "a = a.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[] The ['U.S.', 'Centers', 'for', 'Disease']\n",
      "['The'] U.S. ['Centers', 'for', 'Disease', 'Control']\n",
      "['The', 'U.S.'] Centers ['for', 'Disease', 'Control', 'and']\n",
      "['The', 'U.S.', 'Centers'] for ['Disease', 'Control', 'and', 'Prevention']\n",
      "['The', 'U.S.', 'Centers', 'for'] Disease ['Control', 'and', 'Prevention', 'initially']\n",
      "['U.S.', 'Centers', 'for', 'Disease'] Control ['and', 'Prevention', 'initially', 'advised']\n",
      "['Centers', 'for', 'Disease', 'Control'] and ['Prevention', 'initially', 'advised', 'school']\n",
      "['for', 'Disease', 'Control', 'and'] Prevention ['initially', 'advised', 'school', 'systems']\n",
      "['Disease', 'Control', 'and', 'Prevention'] initially ['advised', 'school', 'systems', 'to']\n",
      "['Control', 'and', 'Prevention', 'initially'] advised ['school', 'systems', 'to', 'close']\n",
      "['and', 'Prevention', 'initially', 'advised'] school ['systems', 'to', 'close', 'if']\n",
      "['Prevention', 'initially', 'advised', 'school'] systems ['to', 'close', 'if', 'outbreaks']\n",
      "['initially', 'advised', 'school', 'systems'] to ['close', 'if', 'outbreaks', 'occurred']\n",
      "['advised', 'school', 'systems', 'to'] close ['if', 'outbreaks', 'occurred', ',']\n",
      "['school', 'systems', 'to', 'close'] if ['outbreaks', 'occurred', ',', 'then']\n",
      "['systems', 'to', 'close', 'if'] outbreaks ['occurred', ',', 'then', 'reversed']\n",
      "['to', 'close', 'if', 'outbreaks'] occurred [',', 'then', 'reversed', 'itself']\n",
      "['close', 'if', 'outbreaks', 'occurred'] , ['then', 'reversed', 'itself', ',']\n",
      "['if', 'outbreaks', 'occurred', ','] then ['reversed', 'itself', ',', 'saying']\n",
      "['outbreaks', 'occurred', ',', 'then'] reversed ['itself', ',', 'saying', 'the']\n",
      "['occurred', ',', 'then', 'reversed'] itself [',', 'saying', 'the', 'apparent']\n",
      "[',', 'then', 'reversed', 'itself'] , ['saying', 'the', 'apparent', 'mildness']\n",
      "['then', 'reversed', 'itself', ','] saying ['the', 'apparent', 'mildness', 'of']\n",
      "['reversed', 'itself', ',', 'saying'] the ['apparent', 'mildness', 'of', 'the']\n",
      "['itself', ',', 'saying', 'the'] apparent ['mildness', 'of', 'the', 'virus']\n",
      "[',', 'saying', 'the', 'apparent'] mildness ['of', 'the', 'virus', 'meant']\n",
      "['saying', 'the', 'apparent', 'mildness'] of ['the', 'virus', 'meant', 'most']\n",
      "['the', 'apparent', 'mildness', 'of'] the ['virus', 'meant', 'most', 'schools']\n",
      "['apparent', 'mildness', 'of', 'the'] virus ['meant', 'most', 'schools', 'and']\n",
      "['mildness', 'of', 'the', 'virus'] meant ['most', 'schools', 'and', 'day']\n",
      "['of', 'the', 'virus', 'meant'] most ['schools', 'and', 'day', 'care']\n",
      "['the', 'virus', 'meant', 'most'] schools ['and', 'day', 'care', 'centers']\n",
      "['virus', 'meant', 'most', 'schools'] and ['day', 'care', 'centers', 'should']\n",
      "['meant', 'most', 'schools', 'and'] day ['care', 'centers', 'should', 'stay']\n",
      "['most', 'schools', 'and', 'day'] care ['centers', 'should', 'stay', 'open']\n",
      "['schools', 'and', 'day', 'care'] centers ['should', 'stay', 'open', ',']\n",
      "['and', 'day', 'care', 'centers'] should ['stay', 'open', ',', 'even']\n",
      "['day', 'care', 'centers', 'should'] stay ['open', ',', 'even', 'if']\n",
      "['care', 'centers', 'should', 'stay'] open [',', 'even', 'if', 'they']\n",
      "['centers', 'should', 'stay', 'open'] , ['even', 'if', 'they', 'had']\n",
      "['should', 'stay', 'open', ','] even ['if', 'they', 'had', 'confirmed']\n",
      "['stay', 'open', ',', 'even'] if ['they', 'had', 'confirmed', 'cases']\n",
      "['open', ',', 'even', 'if'] they ['had', 'confirmed', 'cases', 'of']\n",
      "[',', 'even', 'if', 'they'] had ['confirmed', 'cases', 'of', 'swine']\n",
      "['even', 'if', 'they', 'had'] confirmed ['cases', 'of', 'swine', 'flu']\n",
      "['if', 'they', 'had', 'confirmed'] cases ['of', 'swine', 'flu', '.']\n",
      "['they', 'had', 'confirmed', 'cases'] of ['swine', 'flu', '.']\n",
      "['had', 'confirmed', 'cases', 'of'] swine ['flu', '.']\n",
      "['confirmed', 'cases', 'of', 'swine'] flu ['.']\n",
      "['cases', 'of', 'swine', 'flu'] . []\n"
     ]
    }
   ],
   "source": [
    "left_size = 4\n",
    "right_size = 4\n",
    "word_counts = Counter()\n",
    "for regionText in training_set[:1]:\n",
    "    region = regionText['text'].split()\n",
    "    word_counts.update(region)\n",
    "    for l_context, word, r_context in _context_windows(region, left_size, right_size):\n",
    "        print l_context, word, r_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class GloVeModel():\n",
    "    def __init__(self, embedding_size, context_size, max_vocab_size=100000, min_occurrences=1,\n",
    "                 scaling_factor=3.0/4.0, cooccurrence_cap=100, batch_size=512, learning_rate=0.05):\n",
    "        self.embedding_size = embedding_size\n",
    "        if isinstance(context_size, tuple):\n",
    "            self.left_context, self.right_context = context_size\n",
    "        elif isinstance(context_size, int):\n",
    "            self.left_context = self.right_context = context_size\n",
    "        else:\n",
    "            raise ValueError(\"`context_size` should be an int or a tuple of two ints\")\n",
    "        self.max_vocab_size = max_vocab_size\n",
    "        self.min_occurrences = min_occurrences\n",
    "        self.scaling_factor = scaling_factor\n",
    "        self.cooccurrence_cap = cooccurrence_cap\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.__words = None\n",
    "        self.__word_to_id = None\n",
    "        self.__cooccurrence_matrix = None\n",
    "        self.__embeddings = None\n",
    "\n",
    "    def fit_to_corpus(self, corpus):\n",
    "        self.__fit_to_corpus(corpus, self.max_vocab_size, self.min_occurrences,\n",
    "                             self.left_context, self.right_context)\n",
    "        self.__build_graph()\n",
    "\n",
    "    def __fit_to_corpus(self, corpus, vocab_size, min_occurrences, left_size, right_size):\n",
    "        word_counts = Counter()\n",
    "        cooccurrence_counts = defaultdict(float)\n",
    "        for regionText in corpus:\n",
    "            region = regionText['text'].strip(\",.\").split()\n",
    "            word_counts.update(region)\n",
    "            for l_context, word, r_context in _context_windows(region, left_size, right_size):\n",
    "                for i, context_word in enumerate(l_context[::-1]):\n",
    "                    # add (1 / distance from focal word) for this pair\n",
    "                    cooccurrence_counts[(word, context_word)] += 1 / (i + 1)\n",
    "                for i, context_word in enumerate(r_context):\n",
    "                    cooccurrence_counts[(word, context_word)] += 1 / (i + 1)\n",
    "        if len(cooccurrence_counts) == 0:\n",
    "            raise ValueError(\"No coccurrences in corpus. Did you try to reuse a generator?\")\n",
    "        self.__words = [word for word, count in word_counts.most_common(vocab_size)\n",
    "                        if count >= min_occurrences] \n",
    "        self.__word_to_id = {word: i for i, word in enumerate(self.__words)}\n",
    "        self.__cooccurrence_matrix = {\n",
    "            (self.__word_to_id[words[0]], self.__word_to_id[words[1]]): count\n",
    "            for words, count in cooccurrence_counts.items()\n",
    "            if words[0] in self.__word_to_id and words[1] in self.__word_to_id}\n",
    "\n",
    "    def __build_graph(self):\n",
    "        self.__graph = tf.Graph()\n",
    "        with self.__graph.as_default(), self.__graph.device(_device_for_node):\n",
    "            count_max = tf.constant([self.cooccurrence_cap], dtype=tf.float32,\n",
    "                                    name='max_cooccurrence_count')\n",
    "            scaling_factor = tf.constant([self.scaling_factor], dtype=tf.float32,\n",
    "                                         name=\"scaling_factor\")\n",
    "\n",
    "            self.__focal_input = tf.placeholder(tf.int32, shape=[self.batch_size],\n",
    "                                                name=\"focal_words\")\n",
    "            self.__context_input = tf.placeholder(tf.int32, shape=[self.batch_size],\n",
    "                                                  name=\"context_words\")\n",
    "            self.__cooccurrence_count = tf.placeholder(tf.float32, shape=[self.batch_size],\n",
    "                                                       name=\"cooccurrence_count\")\n",
    "\n",
    "            focal_embeddings = tf.Variable(\n",
    "                tf.random_uniform([self.vocab_size, self.embedding_size], 1.0, -1.0),\n",
    "                name=\"focal_embeddings\")\n",
    "            context_embeddings = tf.Variable(\n",
    "                tf.random_uniform([self.vocab_size, self.embedding_size], 1.0, -1.0),\n",
    "                name=\"context_embeddings\")\n",
    "\n",
    "            focal_biases = tf.Variable(tf.random_uniform([self.vocab_size], 1.0, -1.0),\n",
    "                                       name='focal_biases')\n",
    "            context_biases = tf.Variable(tf.random_uniform([self.vocab_size], 1.0, -1.0),\n",
    "                                         name=\"context_biases\")\n",
    "\n",
    "            focal_embedding = tf.nn.embedding_lookup([focal_embeddings], self.__focal_input)\n",
    "            context_embedding = tf.nn.embedding_lookup([context_embeddings], self.__context_input)\n",
    "            focal_bias = tf.nn.embedding_lookup([focal_biases], self.__focal_input)\n",
    "            context_bias = tf.nn.embedding_lookup([context_biases], self.__context_input)\n",
    "\n",
    "            weighting_factor = tf.minimum(\n",
    "                1.0,\n",
    "                tf.pow(\n",
    "                    tf.div(self.__cooccurrence_count, count_max),\n",
    "                    scaling_factor))\n",
    "\n",
    "            embedding_product = tf.reduce_sum(tf.mul(focal_embedding, context_embedding), 1)\n",
    "\n",
    "            log_cooccurrences = tf.log(tf.to_float(self.__cooccurrence_count))\n",
    "\n",
    "            distance_expr = tf.square(tf.add_n([\n",
    "                embedding_product,\n",
    "                focal_bias,\n",
    "                context_bias,\n",
    "                tf.neg(log_cooccurrences)]))\n",
    "\n",
    "            single_losses = tf.mul(weighting_factor, distance_expr)\n",
    "            self.__total_loss = tf.reduce_sum(single_losses)\n",
    "            tf.scalar_summary(\"GloVe loss\", self.__total_loss)\n",
    "            self.__optimizer = tf.train.AdagradOptimizer(self.learning_rate).minimize(\n",
    "                self.__total_loss)\n",
    "            self.__summary = tf.merge_all_summaries()\n",
    "\n",
    "            self.__combined_embeddings = tf.add(focal_embeddings, context_embeddings,\n",
    "                                                name=\"combined_embeddings\")\n",
    "\n",
    "    def train(self, num_epochs, log_dir=None, summary_batch_interval=1000,\n",
    "              tsne_epoch_interval=None):\n",
    "        should_write_summaries = log_dir is not None and summary_batch_interval\n",
    "        should_generate_tsne = log_dir is not None and tsne_epoch_interval\n",
    "        batches = self.__prepare_batches()\n",
    "        total_steps = 0\n",
    "        with tf.Session(graph=self.__graph) as session:\n",
    "            if should_write_summaries:\n",
    "                summary_writer = tf.train.SummaryWriter(log_dir, graph_def=session.graph_def)\n",
    "            tf.initialize_all_variables().run()\n",
    "            for epoch in range(num_epochs):\n",
    "                shuffle(batches)\n",
    "                for batch_index, batch in enumerate(batches):\n",
    "                    i_s, j_s, counts = batch\n",
    "                    if len(counts) != self.batch_size:\n",
    "                        continue\n",
    "                    feed_dict = {\n",
    "                        self.__focal_input: i_s,\n",
    "                        self.__context_input: j_s,\n",
    "                        self.__cooccurrence_count: counts}\n",
    "                    session.run([self.__optimizer], feed_dict=feed_dict)\n",
    "                    if should_write_summaries and (total_steps + 1) % summary_batch_interval == 0:\n",
    "                        summary_str = session.run(self.__summary, feed_dict=feed_dict)\n",
    "                        summary_writer.add_summary(summary_str, total_steps)\n",
    "                    total_steps += 1\n",
    "                if should_generate_tsne and (epoch + 1) % tsne_epoch_interval == 0:\n",
    "                    current_embeddings = self.__combined_embeddings.eval()\n",
    "                    output_path = os.path.join(log_dir, \"epoch{:03d}.png\".format(epoch + 1))\n",
    "                    self.generate_tsne(output_path, embeddings=current_embeddings)\n",
    "            self.__embeddings = self.__combined_embeddings.eval()\n",
    "            if should_write_summaries:\n",
    "                summary_writer.close()\n",
    "\n",
    "    def embedding_for(self, word_str_or_id):\n",
    "        if isinstance(word_str_or_id, str):\n",
    "            return self.embeddings[self.__word_to_id[word_str_or_id]]\n",
    "        elif isinstance(word_str_or_id, int):\n",
    "            return self.embeddings[word_str_or_id]\n",
    "\n",
    "    def tell_id(self,word):\n",
    "        return self.__word_to_id[word]\n",
    "    \n",
    "    \n",
    "    def containWord(self,word):\n",
    "        if(word in self.__word_to_id):\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "        \n",
    "        \n",
    "    def __prepare_batches(self):\n",
    "        if self.__cooccurrence_matrix is None:\n",
    "            raise NotFitToCorpusError(\n",
    "                \"Need to fit model to corpus before preparing training batches.\")\n",
    "        cooccurrences = [(word_ids[0], word_ids[1], count)\n",
    "                         for word_ids, count in self.__cooccurrence_matrix.items()]\n",
    "        i_indices, j_indices, counts = zip(*cooccurrences)\n",
    "        return list(_batchify(self.batch_size, i_indices, j_indices, counts))\n",
    "\n",
    "    @property\n",
    "    def vocab_size(self):\n",
    "        return len(self.__words)\n",
    "\n",
    "    @property\n",
    "    def words(self):\n",
    "        if self.__words is None:\n",
    "            raise NotFitToCorpusError(\"Need to fit model to corpus before accessing words.\")\n",
    "        return self.__words\n",
    "\n",
    "    @property\n",
    "    def embeddings(self):\n",
    "        if self.__embeddings is None:\n",
    "            raise NotTrainedError(\"Need to train model before accessing embeddings\")\n",
    "        return self.__embeddings\n",
    "\n",
    "    def id_for_word(self, word):\n",
    "        if self.__word_to_id is None:\n",
    "            raise NotFitToCorpusError(\"Need to fit model to corpus before looking up word ids.\")\n",
    "        return self.__word_to_id[word]\n",
    "\n",
    "    def generate_tsne(self, path=None, size=(100, 100), word_count=1000, embeddings=None):\n",
    "        if embeddings is None:\n",
    "            embeddings = self.embeddings\n",
    "        from sklearn.manifold import TSNE\n",
    "        tsne = TSNE(perplexity=30, n_components=2, init='pca', n_iter=5000)\n",
    "        low_dim_embs = tsne.fit_transform(embeddings[:word_count, :])\n",
    "        labels = self.words[:word_count]\n",
    "        return _plot_with_labels(low_dim_embs, labels, path, size)\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _context_windows(region, left_size, right_size):\n",
    "    for i, word in enumerate(region):\n",
    "        start_index = i - left_size\n",
    "        end_index = i + right_size\n",
    "        left_context = _window(region, start_index, i - 1)\n",
    "        right_context = _window(region, i + 1, end_index)\n",
    "        yield (left_context, word, right_context)\n",
    "\n",
    "\n",
    "def _window(region, start_index, end_index):\n",
    "    \"\"\"\n",
    "    Returns the list of words starting from `start_index`, going to `end_index`\n",
    "    taken from region. If `start_index` is a negative number, or if `end_index`\n",
    "    is greater than the index of the last word in region, this function will pad\n",
    "    its return value with `NULL_WORD`.\n",
    "    \"\"\"\n",
    "    last_index = len(region) + 1\n",
    "    selected_tokens = region[max(start_index, 0):min(end_index, last_index) + 1]\n",
    "    return selected_tokens\n",
    "\n",
    "\n",
    "def _device_for_node(n):\n",
    "    if n.type == \"MatMul\":\n",
    "        return \"/gpu:0\"\n",
    "    else:\n",
    "        return \"/cpu:0\"\n",
    "\n",
    "\n",
    "def _batchify(batch_size, *sequences):\n",
    "    for i in xrange(0, len(sequences[0]), batch_size):\n",
    "        yield tuple(sequence[i:i+batch_size] for sequence in sequences)\n",
    "\n",
    "\n",
    "def _plot_with_labels(low_dim_embs, labels, path, size):\n",
    "    import matplotlib.pyplot as plt\n",
    "    assert low_dim_embs.shape[0] >= len(labels), \"More labels than embeddings\"\n",
    "    figure = plt.figure(figsize=size)  # in inches\n",
    "    for i, label in enumerate(labels):\n",
    "        x, y = low_dim_embs[i, :]\n",
    "        plt.scatter(x, y)\n",
    "        plt.annotate(label, xy=(x, y), xytext=(5, 2), textcoords='offset points', ha='right',\n",
    "                     va='bottom')\n",
    "    if path is not None:\n",
    "        figure.savefig(path)\n",
    "        plt.close(figure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = GloVeModel(50, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.fit_to_corpus(training_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.train(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = model.embedding_for('virus')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.57340944"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.set_printoptions(linewidth = 1000000, suppress = True)\n",
    "fout = open('/home/qx344/gloveEmbed.txt', 'w')\n",
    "fout.write(str(len(evalVocab)) + ' ' + str(model.embedding_size) + '\\n')\n",
    "for vocab in evalVocab:\n",
    "    fout.write(vocab)\n",
    "    fout.write(' ')\n",
    "    if model.containWord(vocab):\n",
    "    #if vocab in word_to_index_map:  #embedding_for\n",
    "        #print np.array_str(model.get_embedding(word_to_index_map[vocab]).reshape(1,-1))[2:-2]\n",
    "        #fout.write(np.array_str(model.embedding_for(vocab).reshape(1,-1))[2:-2])\n",
    "        fout.write(model.embedding_for(vocab))\n",
    "    else:\n",
    "        for i in range(model.embedding_size):\n",
    "            fout.write('0.0' + ' ')\n",
    "    fout.write('\\n')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.58481336,  0.57340944,  0.31806454, -0.6068185 ,  0.2880601 , -0.09446254,  0.79951704,  1.2294755 ,  0.0762113 , -0.79812741, -0.9644382 , -0.41085178, -1.02306688,  1.18787301, -1.11813414,  0.26721951,  0.47297657,  0.35375893, -1.41823208, -0.79700112, -1.4817158 , -1.15799618,  1.32393038, -0.20639223, -0.05705154,  0.53559887, -0.32373905, -0.74698293,  0.6458782 ,  0.06923535, -0.49338272,  1.14246178,  1.49533963,  0.23554997,  0.3804363 , -0.30130619,  0.35470659,  0.4408406 , -1.07319152,  0.92572415, -0.97303712, -1.02625942,  0.13298917,  0.40031284, -0.36523563,  0.23910843,  0.49115449,  0.84107864,  1.51430702,  0.42416972], dtype=float32)"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def readEvalVocab(path):\n",
    "    Vocab = []\n",
    "    with open(path) as f:\n",
    "        for i, line in enumerate(f):\n",
    "            Vocab.append(line[:-2])\n",
    "            #print(line[:-1])\n",
    "    return Vocab\n",
    "evalVocab = readEvalVocab('/scratch/qx344/data5/training-data/evalVocab.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'The U.S. Centers for Disease Control and Prevention initially advised school systems to close if outbreaks occurred , then reversed itself , saying the apparent mildness of the virus meant most schools and day care centers should stay open , even if they had confirmed cases of swine flu .\\n'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
