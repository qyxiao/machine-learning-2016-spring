{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cPickle as pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Analyze Stratified Results\n",
    "Laying groundwork to share the methods we used for model selection and parameter tuning.  All accuracy scores will be from cross-validation data.  \n",
    "\n",
    "##Loading data\n",
    "First list all of the parameter variations we were considering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_results_df(results_path):\n",
    "    log = pickle.load(open( results_path, \"rb\" ))\n",
    "    raw_df = pd.DataFrame.from_dict(log)\n",
    "    df = raw_df[['strat_column','strat_value','model','best_score','train_accuracy','test_accuracy','num_cases','num_features','num_opinion_shards']]\n",
    "    return df\n",
    "\n",
    "def print_weighted_accuracy(df):\n",
    "    models = df['model'].unique()\n",
    "    for model in models:\n",
    "        mdf = df.loc[df['model']==model,:]\n",
    "        total_cases = sum(mdf['num_cases'])\n",
    "        weighted_accuracy = sum(mdf['test_accuracy']*mdf['num_cases']/total_cases)\n",
    "        print \"model: %s, weighted accuracy: %s%%\" %(model,round(weighted_accuracy*100,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>strat_column</th>\n",
       "      <th>strat_value</th>\n",
       "      <th>model</th>\n",
       "      <th>best_score</th>\n",
       "      <th>train_accuracy</th>\n",
       "      <th>test_accuracy</th>\n",
       "      <th>num_cases</th>\n",
       "      <th>num_features</th>\n",
       "      <th>num_opinion_shards</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0 </th>\n",
       "      <td> geniss</td>\n",
       "      <td> 0</td>\n",
       "      <td>        baseline</td>\n",
       "      <td>      NaN</td>\n",
       "      <td> 1.000000</td>\n",
       "      <td> 1.000000</td>\n",
       "      <td>   45</td>\n",
       "      <td> 20401</td>\n",
       "      <td> 500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1 </th>\n",
       "      <td> geniss</td>\n",
       "      <td> 0</td>\n",
       "      <td>     naive_bayes</td>\n",
       "      <td>      NaN</td>\n",
       "      <td> 1.000000</td>\n",
       "      <td> 1.000000</td>\n",
       "      <td>   45</td>\n",
       "      <td> 20401</td>\n",
       "      <td> 500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2 </th>\n",
       "      <td> geniss</td>\n",
       "      <td> 0</td>\n",
       "      <td> bernoulli_bayes</td>\n",
       "      <td>      NaN</td>\n",
       "      <td> 1.000000</td>\n",
       "      <td> 1.000000</td>\n",
       "      <td>   45</td>\n",
       "      <td> 20401</td>\n",
       "      <td> 500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3 </th>\n",
       "      <td> geniss</td>\n",
       "      <td> 0</td>\n",
       "      <td>        logistic</td>\n",
       "      <td>      NaN</td>\n",
       "      <td> 1.000000</td>\n",
       "      <td> 1.000000</td>\n",
       "      <td>   45</td>\n",
       "      <td> 20401</td>\n",
       "      <td> 500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4 </th>\n",
       "      <td> geniss</td>\n",
       "      <td> 0</td>\n",
       "      <td>             svm</td>\n",
       "      <td>      NaN</td>\n",
       "      <td> 1.000000</td>\n",
       "      <td> 1.000000</td>\n",
       "      <td>   45</td>\n",
       "      <td> 20401</td>\n",
       "      <td> 500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5 </th>\n",
       "      <td> geniss</td>\n",
       "      <td> 1</td>\n",
       "      <td>        baseline</td>\n",
       "      <td>      NaN</td>\n",
       "      <td> 0.749422</td>\n",
       "      <td> 0.748268</td>\n",
       "      <td> 1730</td>\n",
       "      <td> 20401</td>\n",
       "      <td> 500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6 </th>\n",
       "      <td> geniss</td>\n",
       "      <td> 1</td>\n",
       "      <td>     naive_bayes</td>\n",
       "      <td> 0.749422</td>\n",
       "      <td> 0.749422</td>\n",
       "      <td> 0.748268</td>\n",
       "      <td> 1730</td>\n",
       "      <td> 20401</td>\n",
       "      <td> 500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7 </th>\n",
       "      <td> geniss</td>\n",
       "      <td> 1</td>\n",
       "      <td> bernoulli_bayes</td>\n",
       "      <td> 0.749422</td>\n",
       "      <td> 0.750193</td>\n",
       "      <td> 0.748268</td>\n",
       "      <td> 1730</td>\n",
       "      <td> 20401</td>\n",
       "      <td> 500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8 </th>\n",
       "      <td> geniss</td>\n",
       "      <td> 1</td>\n",
       "      <td>        logistic</td>\n",
       "      <td> 0.749422</td>\n",
       "      <td> 0.749422</td>\n",
       "      <td> 0.748268</td>\n",
       "      <td> 1730</td>\n",
       "      <td> 20401</td>\n",
       "      <td> 500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9 </th>\n",
       "      <td> geniss</td>\n",
       "      <td> 1</td>\n",
       "      <td>             svm</td>\n",
       "      <td> 0.749422</td>\n",
       "      <td> 0.749422</td>\n",
       "      <td> 0.748268</td>\n",
       "      <td> 1730</td>\n",
       "      <td> 20401</td>\n",
       "      <td> 500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td> geniss</td>\n",
       "      <td> 2</td>\n",
       "      <td>        baseline</td>\n",
       "      <td>      NaN</td>\n",
       "      <td> 0.552273</td>\n",
       "      <td> 0.462585</td>\n",
       "      <td>  587</td>\n",
       "      <td> 20401</td>\n",
       "      <td> 500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td> geniss</td>\n",
       "      <td> 2</td>\n",
       "      <td>     naive_bayes</td>\n",
       "      <td> 0.554545</td>\n",
       "      <td> 0.565909</td>\n",
       "      <td> 0.462585</td>\n",
       "      <td>  587</td>\n",
       "      <td> 20401</td>\n",
       "      <td> 500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td> geniss</td>\n",
       "      <td> 2</td>\n",
       "      <td> bernoulli_bayes</td>\n",
       "      <td> 0.568182</td>\n",
       "      <td> 0.843182</td>\n",
       "      <td> 0.530612</td>\n",
       "      <td>  587</td>\n",
       "      <td> 20401</td>\n",
       "      <td> 500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td> geniss</td>\n",
       "      <td> 2</td>\n",
       "      <td>        logistic</td>\n",
       "      <td> 0.572727</td>\n",
       "      <td> 0.997727</td>\n",
       "      <td> 0.544218</td>\n",
       "      <td>  587</td>\n",
       "      <td> 20401</td>\n",
       "      <td> 500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td> geniss</td>\n",
       "      <td> 2</td>\n",
       "      <td>             svm</td>\n",
       "      <td> 0.579545</td>\n",
       "      <td> 0.997727</td>\n",
       "      <td> 0.544218</td>\n",
       "      <td>  587</td>\n",
       "      <td> 20401</td>\n",
       "      <td> 500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td> geniss</td>\n",
       "      <td> 3</td>\n",
       "      <td>        baseline</td>\n",
       "      <td>      NaN</td>\n",
       "      <td> 0.464789</td>\n",
       "      <td> 0.500000</td>\n",
       "      <td>   95</td>\n",
       "      <td> 20401</td>\n",
       "      <td> 500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td> geniss</td>\n",
       "      <td> 3</td>\n",
       "      <td>     naive_bayes</td>\n",
       "      <td> 0.422535</td>\n",
       "      <td> 0.816901</td>\n",
       "      <td> 0.541667</td>\n",
       "      <td>   95</td>\n",
       "      <td> 20401</td>\n",
       "      <td> 500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td> geniss</td>\n",
       "      <td> 3</td>\n",
       "      <td> bernoulli_bayes</td>\n",
       "      <td> 0.478873</td>\n",
       "      <td> 0.788732</td>\n",
       "      <td> 0.500000</td>\n",
       "      <td>   95</td>\n",
       "      <td> 20401</td>\n",
       "      <td> 500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td> geniss</td>\n",
       "      <td> 3</td>\n",
       "      <td>        logistic</td>\n",
       "      <td> 0.478873</td>\n",
       "      <td> 0.845070</td>\n",
       "      <td> 0.583333</td>\n",
       "      <td>   95</td>\n",
       "      <td> 20401</td>\n",
       "      <td> 500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td> geniss</td>\n",
       "      <td> 3</td>\n",
       "      <td>             svm</td>\n",
       "      <td> 0.464789</td>\n",
       "      <td> 0.478873</td>\n",
       "      <td> 0.500000</td>\n",
       "      <td>   95</td>\n",
       "      <td> 20401</td>\n",
       "      <td> 500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td> geniss</td>\n",
       "      <td> 4</td>\n",
       "      <td>        baseline</td>\n",
       "      <td>      NaN</td>\n",
       "      <td> 0.568627</td>\n",
       "      <td> 0.529412</td>\n",
       "      <td>   68</td>\n",
       "      <td> 20401</td>\n",
       "      <td> 500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td> geniss</td>\n",
       "      <td> 4</td>\n",
       "      <td>     naive_bayes</td>\n",
       "      <td> 0.568627</td>\n",
       "      <td> 0.588235</td>\n",
       "      <td> 0.529412</td>\n",
       "      <td>   68</td>\n",
       "      <td> 20401</td>\n",
       "      <td> 500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td> geniss</td>\n",
       "      <td> 4</td>\n",
       "      <td> bernoulli_bayes</td>\n",
       "      <td> 0.568627</td>\n",
       "      <td> 0.627451</td>\n",
       "      <td> 0.529412</td>\n",
       "      <td>   68</td>\n",
       "      <td> 20401</td>\n",
       "      <td> 500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td> geniss</td>\n",
       "      <td> 4</td>\n",
       "      <td>        logistic</td>\n",
       "      <td> 0.647059</td>\n",
       "      <td> 1.000000</td>\n",
       "      <td> 0.647059</td>\n",
       "      <td>   68</td>\n",
       "      <td> 20401</td>\n",
       "      <td> 500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td> geniss</td>\n",
       "      <td> 4</td>\n",
       "      <td>             svm</td>\n",
       "      <td> 0.693878</td>\n",
       "      <td> 1.000000</td>\n",
       "      <td> 0.588235</td>\n",
       "      <td>   68</td>\n",
       "      <td> 20401</td>\n",
       "      <td> 500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td> geniss</td>\n",
       "      <td> 6</td>\n",
       "      <td>        baseline</td>\n",
       "      <td>      NaN</td>\n",
       "      <td> 0.559055</td>\n",
       "      <td> 0.476562</td>\n",
       "      <td>  509</td>\n",
       "      <td> 20401</td>\n",
       "      <td> 500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td> geniss</td>\n",
       "      <td> 6</td>\n",
       "      <td>     naive_bayes</td>\n",
       "      <td> 0.559055</td>\n",
       "      <td> 0.559055</td>\n",
       "      <td> 0.476562</td>\n",
       "      <td>  509</td>\n",
       "      <td> 20401</td>\n",
       "      <td> 500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td> geniss</td>\n",
       "      <td> 6</td>\n",
       "      <td> bernoulli_bayes</td>\n",
       "      <td> 0.564304</td>\n",
       "      <td> 0.758530</td>\n",
       "      <td> 0.476562</td>\n",
       "      <td>  509</td>\n",
       "      <td> 20401</td>\n",
       "      <td> 500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td> geniss</td>\n",
       "      <td> 6</td>\n",
       "      <td>        logistic</td>\n",
       "      <td> 0.566929</td>\n",
       "      <td> 0.706037</td>\n",
       "      <td> 0.476562</td>\n",
       "      <td>  509</td>\n",
       "      <td> 20401</td>\n",
       "      <td> 500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td> geniss</td>\n",
       "      <td> 6</td>\n",
       "      <td>             svm</td>\n",
       "      <td> 0.566929</td>\n",
       "      <td> 0.671916</td>\n",
       "      <td> 0.476562</td>\n",
       "      <td>  509</td>\n",
       "      <td> 20401</td>\n",
       "      <td> 500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td> geniss</td>\n",
       "      <td> 7</td>\n",
       "      <td>        baseline</td>\n",
       "      <td>      NaN</td>\n",
       "      <td> 0.419689</td>\n",
       "      <td> 0.348865</td>\n",
       "      <td> 3346</td>\n",
       "      <td> 20401</td>\n",
       "      <td> 500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td> geniss</td>\n",
       "      <td> 7</td>\n",
       "      <td>     naive_bayes</td>\n",
       "      <td> 0.453169</td>\n",
       "      <td> 0.644081</td>\n",
       "      <td> 0.395460</td>\n",
       "      <td> 3346</td>\n",
       "      <td> 20401</td>\n",
       "      <td> 500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td> geniss</td>\n",
       "      <td> 7</td>\n",
       "      <td> bernoulli_bayes</td>\n",
       "      <td> 0.481068</td>\n",
       "      <td> 0.766441</td>\n",
       "      <td> 0.434886</td>\n",
       "      <td> 3346</td>\n",
       "      <td> 20401</td>\n",
       "      <td> 500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td> geniss</td>\n",
       "      <td> 7</td>\n",
       "      <td>        logistic</td>\n",
       "      <td> 0.471104</td>\n",
       "      <td> 0.799522</td>\n",
       "      <td> 0.426523</td>\n",
       "      <td> 3346</td>\n",
       "      <td> 20401</td>\n",
       "      <td> 500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td> geniss</td>\n",
       "      <td> 7</td>\n",
       "      <td>             svm</td>\n",
       "      <td> 0.473495</td>\n",
       "      <td> 0.794340</td>\n",
       "      <td> 0.427718</td>\n",
       "      <td> 3346</td>\n",
       "      <td> 20401</td>\n",
       "      <td> 500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td> geniss</td>\n",
       "      <td> 9</td>\n",
       "      <td>        baseline</td>\n",
       "      <td>      NaN</td>\n",
       "      <td> 0.418301</td>\n",
       "      <td> 0.450980</td>\n",
       "      <td>  204</td>\n",
       "      <td> 20401</td>\n",
       "      <td> 500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td> geniss</td>\n",
       "      <td> 9</td>\n",
       "      <td>     naive_bayes</td>\n",
       "      <td> 0.437908</td>\n",
       "      <td> 0.653595</td>\n",
       "      <td> 0.450980</td>\n",
       "      <td>  204</td>\n",
       "      <td> 20401</td>\n",
       "      <td> 500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td> geniss</td>\n",
       "      <td> 9</td>\n",
       "      <td> bernoulli_bayes</td>\n",
       "      <td> 0.418301</td>\n",
       "      <td> 0.418301</td>\n",
       "      <td> 0.450980</td>\n",
       "      <td>  204</td>\n",
       "      <td> 20401</td>\n",
       "      <td> 500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td> geniss</td>\n",
       "      <td> 9</td>\n",
       "      <td>        logistic</td>\n",
       "      <td> 0.542484</td>\n",
       "      <td> 1.000000</td>\n",
       "      <td> 0.372549</td>\n",
       "      <td>  204</td>\n",
       "      <td> 20401</td>\n",
       "      <td> 500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td> geniss</td>\n",
       "      <td> 9</td>\n",
       "      <td>             svm</td>\n",
       "      <td> 0.522876</td>\n",
       "      <td> 1.000000</td>\n",
       "      <td> 0.352941</td>\n",
       "      <td>  204</td>\n",
       "      <td> 20401</td>\n",
       "      <td> 500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   strat_column  strat_value            model  best_score  train_accuracy  \\\n",
       "0        geniss            0         baseline         NaN        1.000000   \n",
       "1        geniss            0      naive_bayes         NaN        1.000000   \n",
       "2        geniss            0  bernoulli_bayes         NaN        1.000000   \n",
       "3        geniss            0         logistic         NaN        1.000000   \n",
       "4        geniss            0              svm         NaN        1.000000   \n",
       "5        geniss            1         baseline         NaN        0.749422   \n",
       "6        geniss            1      naive_bayes    0.749422        0.749422   \n",
       "7        geniss            1  bernoulli_bayes    0.749422        0.750193   \n",
       "8        geniss            1         logistic    0.749422        0.749422   \n",
       "9        geniss            1              svm    0.749422        0.749422   \n",
       "10       geniss            2         baseline         NaN        0.552273   \n",
       "11       geniss            2      naive_bayes    0.554545        0.565909   \n",
       "12       geniss            2  bernoulli_bayes    0.568182        0.843182   \n",
       "13       geniss            2         logistic    0.572727        0.997727   \n",
       "14       geniss            2              svm    0.579545        0.997727   \n",
       "15       geniss            3         baseline         NaN        0.464789   \n",
       "16       geniss            3      naive_bayes    0.422535        0.816901   \n",
       "17       geniss            3  bernoulli_bayes    0.478873        0.788732   \n",
       "18       geniss            3         logistic    0.478873        0.845070   \n",
       "19       geniss            3              svm    0.464789        0.478873   \n",
       "20       geniss            4         baseline         NaN        0.568627   \n",
       "21       geniss            4      naive_bayes    0.568627        0.588235   \n",
       "22       geniss            4  bernoulli_bayes    0.568627        0.627451   \n",
       "23       geniss            4         logistic    0.647059        1.000000   \n",
       "24       geniss            4              svm    0.693878        1.000000   \n",
       "25       geniss            6         baseline         NaN        0.559055   \n",
       "26       geniss            6      naive_bayes    0.559055        0.559055   \n",
       "27       geniss            6  bernoulli_bayes    0.564304        0.758530   \n",
       "28       geniss            6         logistic    0.566929        0.706037   \n",
       "29       geniss            6              svm    0.566929        0.671916   \n",
       "30       geniss            7         baseline         NaN        0.419689   \n",
       "31       geniss            7      naive_bayes    0.453169        0.644081   \n",
       "32       geniss            7  bernoulli_bayes    0.481068        0.766441   \n",
       "33       geniss            7         logistic    0.471104        0.799522   \n",
       "34       geniss            7              svm    0.473495        0.794340   \n",
       "35       geniss            9         baseline         NaN        0.418301   \n",
       "36       geniss            9      naive_bayes    0.437908        0.653595   \n",
       "37       geniss            9  bernoulli_bayes    0.418301        0.418301   \n",
       "38       geniss            9         logistic    0.542484        1.000000   \n",
       "39       geniss            9              svm    0.522876        1.000000   \n",
       "\n",
       "    test_accuracy  num_cases  num_features  num_opinion_shards  \n",
       "0        1.000000         45         20401                 500  \n",
       "1        1.000000         45         20401                 500  \n",
       "2        1.000000         45         20401                 500  \n",
       "3        1.000000         45         20401                 500  \n",
       "4        1.000000         45         20401                 500  \n",
       "5        0.748268       1730         20401                 500  \n",
       "6        0.748268       1730         20401                 500  \n",
       "7        0.748268       1730         20401                 500  \n",
       "8        0.748268       1730         20401                 500  \n",
       "9        0.748268       1730         20401                 500  \n",
       "10       0.462585        587         20401                 500  \n",
       "11       0.462585        587         20401                 500  \n",
       "12       0.530612        587         20401                 500  \n",
       "13       0.544218        587         20401                 500  \n",
       "14       0.544218        587         20401                 500  \n",
       "15       0.500000         95         20401                 500  \n",
       "16       0.541667         95         20401                 500  \n",
       "17       0.500000         95         20401                 500  \n",
       "18       0.583333         95         20401                 500  \n",
       "19       0.500000         95         20401                 500  \n",
       "20       0.529412         68         20401                 500  \n",
       "21       0.529412         68         20401                 500  \n",
       "22       0.529412         68         20401                 500  \n",
       "23       0.647059         68         20401                 500  \n",
       "24       0.588235         68         20401                 500  \n",
       "25       0.476562        509         20401                 500  \n",
       "26       0.476562        509         20401                 500  \n",
       "27       0.476562        509         20401                 500  \n",
       "28       0.476562        509         20401                 500  \n",
       "29       0.476562        509         20401                 500  \n",
       "30       0.348865       3346         20401                 500  \n",
       "31       0.395460       3346         20401                 500  \n",
       "32       0.434886       3346         20401                 500  \n",
       "33       0.426523       3346         20401                 500  \n",
       "34       0.427718       3346         20401                 500  \n",
       "35       0.450980        204         20401                 500  \n",
       "36       0.450980        204         20401                 500  \n",
       "37       0.450980        204         20401                 500  \n",
       "38       0.372549        204         20401                 500  \n",
       "39       0.352941        204         20401                 500  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df = get_results_df(\"../results/model_results.pkl.20150510-013946.20150510-013946.min_required_count.50.all_features.accuracy.stratified\")\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: baseline, weighted accuracy: 48.5%\n",
      "model: naive_bayes, weighted accuracy: 51.0%\n",
      "model: bernoulli_bayes, weighted accuracy: 53.5%\n",
      "model: logistic, weighted accuracy: 53.2%\n",
      "model: svm, weighted accuracy: 53.0%\n"
     ]
    }
   ],
   "source": [
    "print_weighted_accuracy(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiAAAAFQCAYAAACGQ8VAAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nkiRJkiRJ6sL/Bw3ALej1AhRIAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10a401c90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "CONTEXT='notebook'\n",
    "font_size = {\n",
    "    'paper':8,\n",
    "    'poster':16,\n",
    "    'notebook':10,\n",
    "    'talk':13\n",
    "}\n",
    "def chart_weighted_accuracy(df,metric):\n",
    "    baseline_scores = {'best_score':'train_accuracy','test_accuracy':'test_accuracy'}\n",
    "    models = df['model'].unique()\n",
    "    scores = []\n",
    "    for model in models:\n",
    "        mdf = df.loc[df['model']==model,:]\n",
    "        total_cases = sum(mdf['num_cases'])\n",
    "        \n",
    "        if model=='baseline':\n",
    "            weighted_accuracy = sum(mdf[baseline_scores[metric]]*mdf['num_cases']/total_cases)\n",
    "        else:\n",
    "            weighted_accuracy = sum(mdf[metric]*mdf['num_cases']/total_cases)\n",
    "        scores.append(weighted_accuracy)\n",
    "            \n",
    "    return models,scores\n",
    "        \n",
    "def weighted_accuracy_bars(df,metric,context):\n",
    "    '''\n",
    "    df: data frame\n",
    "    context: paper,talk, notebook, poster\n",
    "    '''\n",
    "    \n",
    "    sns.set_context(context)\n",
    "    model_list,score_list = chart_weighted_accuracy(df,metric)\n",
    "    \n",
    "    #size and position of bars\n",
    "    bar_pos = np.arange(len(model_list))\n",
    "    bar_size = score_list\n",
    "    bar_labels = model_list\n",
    "    \n",
    "    #plot\n",
    "    plt.barh(bar_pos,bar_size, align='center', alpha=0.4)\n",
    "    plt.yticks(bar_pos, bar_labels)\n",
    "    plt.xticks([],[]) #no x-axis\n",
    "\n",
    "    #Add data labels\n",
    "    for x,y in zip(bar_size,bar_pos):\n",
    "        plt.text(x+0.01, y, '%.2f' % x, ha='left', va='center',fontsize=font_size[context])\n",
    "        \n",
    "    pretty_metric = {'test_accuracy':'Test','best_score':'CV'}\n",
    "    plt.title('Optimized %s Accuracy of Each Model' % pretty_metric[metric])\n",
    "    plt.savefig('foo.png', bbox_inches='tight')\n",
    "    \n",
    "weighted_accuracy_bars(results_df,'test_accuracy',CONTEXT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Same results, but with Mixed and Unknown Valences removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "results_df = get_results_df(\"../results/model_results.pkl.20150510-022044.20150510-022044.min_required_count.50.all_features.accuracy\")\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print_weighted_accuracy(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "STRAT_COLUMN=None\n",
    "print 'stratify_by_'+STRAT_COLUMN if STRAT_COLUMN else ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_weighted_accuracy(df):\n",
    "    models = df['model'].unique()\n",
    "    for model in models:\n",
    "        mdf = df.loc[df['model']==model,:]\n",
    "        total_cases = sum(mdf['num_cases'])\n",
    "        weighted_accuracy = sum(mdf['test_accuracy']*mdf['num_cases']/total_cases)\n",
    "        print \"model: %s, weighted accuracy: %s%%\" %(model,round(weighted_accuracy*100,1))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
