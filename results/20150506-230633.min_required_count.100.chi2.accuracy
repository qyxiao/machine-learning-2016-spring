Experiment: 20150506-230633.min_required_count.100.chi2.accuracy
Data parameters:
  Number of opinion shards: 1340
  Minimum required count: 100
  Using TF-IDF: True
Data loaded from /scratch/akp258/ml_output_data/feature_matrix.svmlight.shards.1340.mincount.100.tfidf and /scratch/akp258/ml_output_data/case_ids.shards.p.1340.mincount.100.tfidf
Total time spent building data: 14.1483318806
Training and scoring models...

Fitting New Model
Model: baseline
Feature Matrix Info:
  Number of cases 15466
  Number of features 23534
Training percentage 0.75
Scoring used: None
Splitting data into training and testing...
Total time: 0.0924909114838
Training Accuracy
	 	 pred
true 	 	 1 	 2 	 3
	 1 	 5651 	 0 	 0
	 2 	 1840 	 0 	 0
	 3 	 4108 	 0 	 0
Percent Accuracy: 48.720%
Testing Accuracy
	 	 pred
true 	 	 1 	 2 	 3
	 1 	 2154 	 0 	 0
	 2 	 627 	 0 	 0
	 3 	 1086 	 0 	 0
Percent Accuracy: 55.702%
Log file doesn't exist.  Creating from scratch.

Fitting New Model
Model: logistic
Feature Matrix Info:
  Number of cases 15466
  Number of features 23534
Training percentage 0.75
Scoring used: accuracy
Regularization bounded between 10^(-2) and 10^(2):
Splitting data into training and testing...
Running Model Pipeline...
Fitting 3 folds for each of 20 candidates, totalling 60 fits
[Parallel(n_jobs=-1)]: Done   1 jobs       | elapsed:    0.8s
[Parallel(n_jobs=-1)]: Done  60 out of  60 | elapsed:   40.3s finished
Fitting Complete!

best estimator: Pipeline(steps=[('feature_reduction', SelectFpr(alpha=1.0, score_func=<function chi2 at 0x2ba876049140>)), ('classifier', LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='ovr',
          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,
          verbose=0))])
best params: {'classifier__C': 1, 'feature_reduction__alpha': 1.0}
best score from that estimator: 0.559013708078
Total time: 45.9693927765
Training Accuracy
	 	 pred
true 	 	 1 	 2 	 3
	 1 	 4842 	 124 	 685
	 2 	 617 	 855 	 368
	 3 	 1216 	 116 	 2776
Percent Accuracy: 73.049%
Testing Accuracy
	 	 pred
true 	 	 1 	 2 	 3
	 1 	 1751 	 61 	 342
	 2 	 401 	 81 	 145
	 3 	 677 	 47 	 362
Percent Accuracy: 56.736%

Fitting New Model
Model: svm
Feature Matrix Info:
  Number of cases 15466
  Number of features 23534
Training percentage 0.75
Scoring used: accuracy
Regularization bounded between 10^(-2) and 10^(2):
Splitting data into training and testing...
Running Model Pipeline...
Fitting 3 folds for each of 20 candidates, totalling 60 fits
[Parallel(n_jobs=-1)]: Done   1 jobs       | elapsed:    0.8s
[Parallel(n_jobs=-1)]: Done  60 out of  60 | elapsed:  2.4min finished
Fitting Complete!

best estimator: Pipeline(steps=[('feature_reduction', SelectFpr(alpha=0.8, score_func=<function chi2 at 0x2ba876049140>)), ('classifier', LinearSVC(C=0.1, class_weight=None, dual=False, fit_intercept=True,
     intercept_scaling=1, loss='squared_hinge', max_iter=1000,
     multi_class='ovr', penalty='l2', random_state=0, tol=0.0001,
     verbose=0))])
best params: {'classifier__C': 0.1, 'feature_reduction__alpha': 0.8}
best score from that estimator: 0.557979136132
Total time: 144.236145973
Training Accuracy
	 	 pred
true 	 	 1 	 2 	 3
	 1 	 4597 	 160 	 894
	 2 	 729 	 709 	 402
	 3 	 1549 	 145 	 2414
Percent Accuracy: 66.557%
Testing Accuracy
	 	 pred
true 	 	 1 	 2 	 3
	 1 	 1796 	 38 	 320
	 2 	 425 	 64 	 138
	 3 	 703 	 38 	 345
Percent Accuracy: 57.021%

Fitting New Model
Model: naive_bayes
Feature Matrix Info:
  Number of cases 15466
  Number of features 23534
Training percentage 0.75
Scoring used: accuracy
Regularization bounded between 10^(-2) and 10^(2):
Splitting data into training and testing...
Running Model Pipeline...
Fitting 3 folds for each of 4 candidates, totalling 12 fits
[Parallel(n_jobs=-1)]: Done   1 jobs       | elapsed:    0.9s
[Parallel(n_jobs=-1)]: Done   3 out of  12 | elapsed:    1.4s remaining:    4.3s
[Parallel(n_jobs=-1)]: Done  12 out of  12 | elapsed:    3.6s finished
Fitting Complete!

best estimator: Pipeline(steps=[('feature_reduction', SelectFpr(alpha=0.8, score_func=<function chi2 at 0x2ba876049140>)), ('classifier', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))])
best params: {'feature_reduction__alpha': 0.8}
best score from that estimator: 0.54056384171
Total time: 4.22243690491
Training Accuracy
	 	 pred
true 	 	 1 	 2 	 3
	 1 	 4029 	 123 	 1499
	 2 	 652 	 519 	 669
	 3 	 1436 	 88 	 2584
Percent Accuracy: 61.488%
Testing Accuracy
	 	 pred
true 	 	 1 	 2 	 3
	 1 	 1545 	 31 	 578
	 2 	 312 	 63 	 252
	 3 	 537 	 25 	 524
Percent Accuracy: 55.133%

Fitting New Model
Model: bernoulli_bayes
Feature Matrix Info:
  Number of cases 15466
  Number of features 23534
Training percentage 0.75
Scoring used: accuracy
Regularization bounded between 10^(-2) and 10^(2):
Splitting data into training and testing...
Running Model Pipeline...
Fitting 3 folds for each of 8 candidates, totalling 24 fits
[Parallel(n_jobs=-1)]: Done   1 jobs       | elapsed:    0.7s
[Parallel(n_jobs=-1)]: Done   2 out of  24 | elapsed:    1.1s remaining:   12.5s
[Parallel(n_jobs=-1)]: Done  24 out of  24 | elapsed:    6.2s finished
Fitting Complete!

best estimator: Pipeline(steps=[('feature_reduction', SelectFpr(alpha=0.8, score_func=<function chi2 at 0x2ba876049140>)), ('classifier', BernoulliNB(alpha=1.0, binarize=0.1, class_prior=None, fit_prior=True))])
best params: {'classifier__binarize': 0.1, 'feature_reduction__alpha': 0.8}
best score from that estimator: 0.538839555134
Total time: 6.94231081009
Training Accuracy
	 	 pred
true 	 	 1 	 2 	 3
	 1 	 3652 	 555 	 1444
	 2 	 333 	 1121 	 386
	 3 	 1070 	 389 	 2649
Percent Accuracy: 63.988%
Testing Accuracy
	 	 pred
true 	 	 1 	 2 	 3
	 1 	 1356 	 187 	 611
	 2 	 281 	 148 	 198
	 3 	 454 	 138 	 494
Percent Accuracy: 51.668%
